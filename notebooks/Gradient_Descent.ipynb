{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Basic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, start, learn_rate, n_iter=50, tolerance=1e-06\n",
    "):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * gradient(vector)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.210739197207331e-06"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.77519666596786e-07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.050060671375367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.660323412732294"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,\n",
    "    n_iter=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004317124741065828"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,\n",
    "    n_iter=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.952518849647663e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,\n",
    "    n_iter=2000\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function 𝑣⁴ - 5𝑣² - 3𝑣. It has a global minimum in 𝑣 ≈ 1.7 and a local minimum in 𝑣 ≈ −1.42. The gradient of this function is 4𝑣³ − 10𝑣 − 3. Let’s see how gradient_descent() works here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4207567437458342"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,\n",
    "    learn_rate=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.285401330315467"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,\n",
    "    learn_rate=0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the function 𝑣 − log(𝑣). The gradient of this function is 1 − 1/𝑣. With this information, you can find its minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000011077232125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: 1 - 1 / v, start=2.5, learn_rate=0.5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use gradient_descent() with functions of more than one variable. The application is the same, but you need to provide the gradient and starting points as vectors or arrays. For example, you can find the minimum of the function 𝑣₁² + 𝑣₂⁴ that has the gradient vector (2𝑣₁, 4𝑣₂³):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.08281277e-12, 9.75207120e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    gradient=lambda v: np.array([2 * v[0], 4 * v[1]**3]),\n",
    "    start=np.array([1.0, 1.0]), learn_rate=0.2, tolerance=1e-08\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares\n",
    "\n",
    "As you’ve already learned, linear regression and the ordinary least squares method start with the observed values of the inputs 𝐱 = (𝑥₁, …, 𝑥ᵣ) and outputs 𝑦. They define a linear function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ, which is as close as possible to 𝑦.\n",
    "\n",
    "This is an optimization problem. It finds the values of weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ that minimize the sum of squared residuals SSR = Σᵢ(𝑦ᵢ − 𝑓(𝐱ᵢ))² or the mean squared error MSE = SSR / 𝑛. Here, 𝑛 is the total number of observations and 𝑖 = 1, …, 𝑛.\n",
    "\n",
    "You can also use the cost function 𝐶 = SSR / (2𝑛), which is mathematically more convenient than SSR or MSE.\n",
    "\n",
    "The most basic form of linear regression is simple linear regression. It has only one set of inputs 𝑥 and two weights: 𝑏₀ and 𝑏₁. The equation of the regression line is 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥. Although the optimal values of 𝑏₀ and 𝑏₁ can be calculated analytically, you’ll use gradient descent to determine them.\n",
    "\n",
    "First, you need calculus to find the gradient of the cost function 𝐶 = Σᵢ(𝑦ᵢ − 𝑏₀ − 𝑏₁𝑥ᵢ)² / (2𝑛). Since you have two decision variables, 𝑏₀ and 𝑏₁, the gradient ∇𝐶 is a vector with two components:\n",
    "\n",
    "1. ∂𝐶/∂𝑏₀ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) = mean(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ)\n",
    "2. ∂𝐶/∂𝑏₁ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ = mean((𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ)\n",
    "\n",
    "You need the values of 𝑥 and 𝑦 to calculate the gradient of this cost function. Your gradient function will have as inputs not only 𝑏₀ and 𝑏₁ but also 𝑥 and 𝑦. This is how it might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr_gradient(x, y, b):\n",
    "    res = b[0] + b[1] * x - y\n",
    "    return res.mean(), (res * x).mean()  # .mean() is a method of np.ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient_descent() needs two small adjustments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06\n",
    "):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector))\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.62822349, 0.54012867])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55])\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "gradient_descent(\n",
    "    ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,\n",
    "    n_iter=100_000\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement of the Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06,\n",
    "    dtype=\"float64\"\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Recalculating the difference\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)\n",
    "\n",
    "        # Checking if the absolute difference is small enough\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "\n",
    "        # Updating the values of the variables\n",
    "        vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient_descent() now accepts an additional dtype parameter that defines the data type of NumPy arrays inside the function. For more information about NumPy types, see the official documentation on data types.\n",
    "\n",
    "In most applications, you won’t notice a difference between 32-bit and 64-bit floating-point numbers, but when you work with big datasets, this might significantly affect memory use and maybe even processing speed. For example, although NumPy uses 64-bit floats by default, TensorFlow often uses 32-bit decimal numbers.\n",
    "\n",
    "In addition to considering data types, the code above introduces a few modifications related to type checking and ensuring the use of NumPy capabilities:\n",
    "\n",
    "Lines 8 and 9 check if gradient is a Python callable object and whether it can be used as a function. If not, then the function will raise a TypeError.\n",
    "\n",
    "Line 12 sets an instance of numpy.dtype, which will be used as the data type for all arrays throughout the function.\n",
    "\n",
    "Line 15 takes the arguments x and y and produces NumPy arrays with the desired data type. The arguments x and y can be lists, tuples, arrays, or other sequences.\n",
    "\n",
    "Lines 16 and 17 compare the sizes of x and y. This is useful because you want to be sure that both arrays have the same number of observations. If they don’t, then the function will raise a ValueError.\n",
    "\n",
    "Line 20 converts the argument start to a NumPy array. This is an interesting trick: if start is a Python scalar, then it’ll be transformed into a corresponding NumPy object (an array with one item and zero dimensions). If you pass a sequence, then it’ll become a regular NumPy array with the same number of elements.\n",
    "\n",
    "Line 23 does the same thing with the learning rate. This can be very useful because it enables you to specify different learning rates for each decision variable by passing a list, tuple, or NumPy array to gradient_descent().\n",
    "\n",
    "Lines 24 and 25 check if the learning rate value (or values for all variables) is greater than zero.\n",
    "\n",
    "Lines 28 to 35 similarly set n_iter and tolerance and check that they are greater than zero.\n",
    "\n",
    "Lines 38 to 47 are almost the same as before. The only difference is the type of the gradient array on line 40.\n",
    "\n",
    "Line 49 conveniently returns the resulting array if you have several decision variables or a Python scalar if you have a single variable.\n",
    "\n",
    "Your gradient_descent() is now finished. Feel free to add some additional capabilities or polishing. The next step of this tutorial is to use what you’ve learned so far to implement the stochastic version of gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatches in Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sgd(\n",
    "    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,\n",
    "    tolerance=1e-06, dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = -learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.63093736, 0.53982921])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(\n",
    "    ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,\n",
    "    batch_size=3, n_iter=100_000, random_state=0\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum in Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sgd(\n",
    "    gradient, x, y, start, learn_rate=0.1, decay_rate=0.0, batch_size=1,\n",
    "    n_iter=50, tolerance=1e-06, dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the decay rate\n",
    "    decay_rate = np.array(decay_rate, dtype=dtype_)\n",
    "    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n",
    "        raise ValueError(\"'decay_rate' must be between zero and one\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Setting the difference to zero for the first iteration\n",
    "    diff = 0\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = decay_rate * diff - learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Start Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sgd(\n",
    "    gradient, x, y, n_vars=None, start=None, learn_rate=0.1,\n",
    "    decay_rate=0.0, batch_size=1, n_iter=50, tolerance=1e-06,\n",
    "    dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    # Setting up the data type for NumPy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    # Converting x and y to NumPy arrays\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "\n",
    "    # Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing the values of the variables\n",
    "    vector = (\n",
    "        rng.normal(size=int(n_vars)).astype(dtype_)\n",
    "        if start is None else\n",
    "        np.array(start, dtype=dtype_)\n",
    "    )\n",
    "\n",
    "    # Setting up and checking the learning rate\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    if np.any(learn_rate <= 0):\n",
    "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the decay rate\n",
    "    decay_rate = np.array(decay_rate, dtype=dtype_)\n",
    "    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n",
    "        raise ValueError(\"'decay_rate' must be between zero and one\")\n",
    "\n",
    "    # Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    # Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "\n",
    "    # Setting the difference to zero for the first iteration\n",
    "    diff = 0\n",
    "\n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = decay_rate * diff - learn_rate * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.63014443, 0.53901017])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(\n",
    "    ssr_gradient, x, y, n_vars=2, learn_rate=0.0001,\n",
    "    decay_rate=0.8, batch_size=3, n_iter=100_000, random_state=0\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Keras and TensorFlow\n",
    "\n",
    "* Adam\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26908/3067303087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Create needed objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create needed objects\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "var = tf.Variable(2.5)\n",
    "cost = lambda: 2 + var ** 2\n",
    "\n",
    "# Perform optimization\n",
    "for _ in range(100):\n",
    "    sgd.minimize(cost, var_list=[var])\n",
    "\n",
    "# Extract results\n",
    "var.numpy()\n",
    "\n",
    "cost().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dac4372839ee31ea8922f5c01c9ab80fd5aa575c72c1f76bbc3ccff5d54b2c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
